/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to you under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.calcite.adapter.file;

import org.apache.calcite.jdbc.CalciteConnection;
import org.apache.calcite.schema.SchemaPlus;
import org.apache.calcite.util.Sources;

import org.junit.jupiter.api.AfterEach;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Tag;
import org.junit.jupiter.api.Test;
import org.opentest4j.TestAbortedException;

import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Path;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.Statement;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Locale;
import java.util.Map;

/**
 * Test materialized views with Parquet execution engine.
 */
@Tag("unit")
public class MaterializedViewParquetTest {
  private Path tempDir;

  @BeforeEach
  public void setUp() throws Exception {
    // Create temporary directory manually to avoid JUnit automatic cleanup issues
    tempDir = Files.createTempDirectory("materialized-view-test-");
    
    // Create sales data
    File salesCsv = new File(tempDir.toFile(), "sales.csv");
    try (FileWriter writer = new FileWriter(salesCsv, StandardCharsets.UTF_8)) {
      writer.write("date:string,product:string,quantity:int,price:double\n");
      writer.write("2024-01-01,Widget,10,25.50\n");
      writer.write("2024-01-01,Gadget,5,50.00\n");
      writer.write("2024-01-02,Widget,15,25.50\n");
      writer.write("2024-01-02,Gizmo,8,75.00\n");
      writer.write("2024-01-03,Gadget,12,50.00\n");
      writer.write("2024-01-03,Widget,20,25.50\n");
    }

    // Pre-create materialized view Parquet file
    // In a real implementation, this would be created by executing the SQL
    File mvDir = new File(tempDir.toFile(), ".materialized_views");
    mvDir.mkdirs();

    // For testing, we'll create a simple Parquet file with the expected data
    // In reality, this would be generated by executing the MV SQL and writing results
    System.out.println("Created .materialized_views directory: " + mvDir.getAbsolutePath());
  }

  @AfterEach
  public void tearDown() throws Exception {
    // Clear caches after each test to prevent contamination
    try {
      Sources.clearFileCache();
    } catch (Exception e) {
      // Log but don't fail the test for cache cleanup issues
      System.err.println("Warning: Failed to clear file cache during cleanup: " + e.getMessage());
    }
    
    // Force cleanup to help with temp directory deletion
    try {
      forceCleanup();
    } catch (Exception e) {
      // Log but don't fail the test for cleanup issues
      System.err.println("Warning: Failed during resource cleanup: " + e.getMessage());
    }
    
    // Clean up temp directory - failures should never be fatal
    if (tempDir != null) {
      deleteTempDirectoryRecursively(tempDir.toFile());
    }
  }

  /**
   * Force cleanup of resources that might be holding file locks.
   * This helps prevent directory deletion issues in tests.
   * Temp directory deletion failures should never be fatal.
   */
  private void forceCleanup() throws InterruptedException {
    // Multiple rounds of GC to ensure cleanup
    for (int i = 0; i < 3; i++) {
      System.gc();
      Thread.sleep(50);
    }
  }

  /**
   * Check if current engine supports materialized views.
   * Skip test if not using DUCKDB or PARQUET engines.
   */
  private void skipIfEngineDoesNotSupportMaterializedViews() {
    String currentEngine = System.getenv("CALCITE_FILE_ENGINE_TYPE");
    if (currentEngine == null) {
      currentEngine = System.getProperty("CALCITE_FILE_ENGINE_TYPE", "PARQUET");
    }
    
    if (!"PARQUET".equals(currentEngine) && !"DUCKDB".equals(currentEngine)) {
      throw new TestAbortedException("Skipping test - materialized views only supported with PARQUET or DUCKDB engines, current: " + currentEngine);
    }
  }

  /**
   * Recursively delete a directory and all its contents.
   * Failures are logged but never cause test failures.
   * This follows the principle that temp directory deletion should never be fatal.
   */
  private void deleteTempDirectoryRecursively(File directory) {
    if (directory == null || !directory.exists()) {
      return;
    }
    
    try {
      File[] files = directory.listFiles();
      if (files != null) {
        for (File file : files) {
          if (file.isDirectory()) {
            deleteTempDirectoryRecursively(file);
          } else {
            if (!file.delete()) {
              System.err.println("Warning: Could not delete temp file: " + file.getAbsolutePath());
            }
          }
        }
      }
      
      if (!directory.delete()) {
        System.err.println("Warning: Could not delete temp directory: " + directory.getAbsolutePath());
      }
    } catch (Exception e) {
      // Log but never fail the test for temp directory cleanup issues
      System.err.println("Warning: Exception during temp directory cleanup (ignored): " + e.getMessage());
    }
  }

  @Test public void testMaterializedViewsWithParquetEngine() throws Exception {
    skipIfEngineDoesNotSupportMaterializedViews();
    
    System.out.println("\n=== MATERIALIZED VIEWS WITH PARQUET ENGINE TEST ===");

    try (Connection connection = DriverManager.getConnection("jdbc:calcite:");
         CalciteConnection calciteConnection = connection.unwrap(CalciteConnection.class)) {

      SchemaPlus rootSchema = calciteConnection.getRootSchema();

      // Create materialization definitions
      List<Map<String, Object>> materializations = new ArrayList<>();

      Map<String, Object> dailySalesMV = new HashMap<>();
      dailySalesMV.put("view", "daily_summary");
      dailySalesMV.put("table", "daily_summary_mv");
      dailySalesMV.put("sql", "SELECT \"date\", " +
          "COUNT(*) as transaction_count, " +
          "SUM(\"quantity\") as total_quantity, " +
          "SUM(\"quantity\" * \"price\") as total_revenue " +
          "FROM \"sales\" " +
          "GROUP BY \"date\"");
      materializations.add(dailySalesMV);

      // Configure file schema with materializations (engine determined by environment)
      Map<String, Object> operand = new HashMap<>();
      operand.put("directory", tempDir.toString());
      operand.put("materializations", materializations);

      System.out.println("\n1. Creating schema with Parquet engine and materialized view 'daily_summary'");
      SchemaPlus fileSchema =
          rootSchema.add("PARQUET_MV_TEST", FileSchemaFactory.INSTANCE.create(rootSchema, "PARQUET_MV_TEST", operand));

      try (Statement stmt = connection.createStatement()) {
        // Check if .parquet file was expected
        File mvParquetFile = new File(tempDir.toFile(), ".materialized_views/daily_summary_mv.parquet");
        System.out.println("\n2. Checking for materialized view Parquet file:");
        System.out.println("   Expected location: " + mvParquetFile.getAbsolutePath());
        System.out.println("   File exists: " + mvParquetFile.exists());

        // List all available tables
        System.out.println("\n3. Listing all tables in schema:");
        ResultSet tables =
            connection.getMetaData().getTables(null, "PARQUET_MV_TEST", "%", null);

        while (tables.next()) {
          String tableName = tables.getString("TABLE_NAME");
          System.out.println("   - " + tableName);
        }

        // Show that without Parquet engine, we get an error message
        System.out.println("\n4. Testing with non-Parquet engine:");
        Map<String, Object> linq4jOperand = new HashMap<>();
        linq4jOperand.put("directory", tempDir.toString());
        linq4jOperand.put("executionEngine", "linq4j");
        linq4jOperand.put("materializations", materializations);

        System.out.println("   Creating schema with LINQ4J engine and materializations...");
        SchemaPlus linq4jSchema =
            rootSchema.add("LINQ4J_MV_TEST", FileSchemaFactory.INSTANCE.create(rootSchema, "LINQ4J_MV_TEST", linq4jOperand));
        System.out.println("   Expected: Error message that MV only works with Parquet");
      }
    }
  }

}
