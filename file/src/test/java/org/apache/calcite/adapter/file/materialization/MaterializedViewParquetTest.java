/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to you under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.calcite.adapter.file;

import org.apache.calcite.util.Sources;

import org.junit.jupiter.api.AfterEach;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Tag;
import org.junit.jupiter.api.Test;
import org.opentest4j.TestAbortedException;

import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Path;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.Statement;
import java.util.Locale;

/**
 * Test materialized views with Parquet execution engine.
 */
@Tag("unit")
public class MaterializedViewParquetTest {
  private Path tempDir;

  @BeforeEach
  public void setUp() throws Exception {
    // Create temporary directory manually to avoid JUnit automatic cleanup issues
    tempDir = Files.createTempDirectory("materialized-view-test-");
    
    // Create sales data
    File salesCsv = new File(tempDir.toFile(), "sales.csv");
    try (FileWriter writer = new FileWriter(salesCsv, StandardCharsets.UTF_8)) {
      writer.write("date:string,product:string,quantity:int,price:double\n");
      writer.write("2024-01-01,Widget,10,25.50\n");
      writer.write("2024-01-01,Gadget,5,50.00\n");
      writer.write("2024-01-02,Widget,15,25.50\n");
      writer.write("2024-01-02,Gizmo,8,75.00\n");
      writer.write("2024-01-03,Gadget,12,50.00\n");
      writer.write("2024-01-03,Widget,20,25.50\n");
    }

    // Pre-create materialized view Parquet file
    // In a real implementation, this would be created by executing the SQL
    File mvDir = new File(tempDir.toFile(), ".materialized_views");
    mvDir.mkdirs();

    // For testing, we'll create a simple Parquet file with the expected data
    // In reality, this would be generated by executing the MV SQL and writing results
    System.out.println("Created .materialized_views directory: " + mvDir.getAbsolutePath());
  }

  @AfterEach
  public void tearDown() throws Exception {
    // Clear caches after each test to prevent contamination
    try {
      Sources.clearFileCache();
    } catch (Exception e) {
      // Log but don't fail the test for cache cleanup issues
      System.err.println("Warning: Failed to clear file cache during cleanup: " + e.getMessage());
    }
    
    // Force cleanup to help with temp directory deletion
    try {
      forceCleanup();
    } catch (Exception e) {
      // Log but don't fail the test for cleanup issues
      System.err.println("Warning: Failed during resource cleanup: " + e.getMessage());
    }
    
    // Clean up temp directory - failures should never be fatal
    if (tempDir != null) {
      deleteTempDirectoryRecursively(tempDir.toFile());
    }
  }

  /**
   * Force cleanup of resources that might be holding file locks.
   * This helps prevent directory deletion issues in tests.
   * Temp directory deletion failures should never be fatal.
   */
  private void forceCleanup() throws InterruptedException {
    // Multiple rounds of GC to ensure cleanup
    for (int i = 0; i < 3; i++) {
      System.gc();
      Thread.sleep(50);
    }
  }

  /**
   * Check if current engine supports materialized views.
   * Skip test if not using DUCKDB or PARQUET engines.
   */
  private void skipIfEngineDoesNotSupportMaterializedViews() {
    String currentEngine = System.getenv("CALCITE_FILE_ENGINE_TYPE");
    if (currentEngine == null) {
      currentEngine = System.getProperty("CALCITE_FILE_ENGINE_TYPE", "PARQUET");
    }
    
    if (!"PARQUET".equals(currentEngine) && !"DUCKDB".equals(currentEngine)) {
      throw new TestAbortedException("Skipping test - materialized views only supported with PARQUET or DUCKDB engines, current: " + currentEngine);
    }
  }

  /**
   * Recursively delete a directory and all its contents.
   * Failures are logged but never cause test failures.
   * This follows the principle that temp directory deletion should never be fatal.
   */
  private void deleteTempDirectoryRecursively(File directory) {
    if (directory == null || !directory.exists()) {
      return;
    }
    
    try {
      File[] files = directory.listFiles();
      if (files != null) {
        for (File file : files) {
          if (file.isDirectory()) {
            deleteTempDirectoryRecursively(file);
          } else {
            if (!file.delete()) {
              System.err.println("Warning: Could not delete temp file: " + file.getAbsolutePath());
            }
          }
        }
      }
      
      if (!directory.delete()) {
        System.err.println("Warning: Could not delete temp directory: " + directory.getAbsolutePath());
      }
    } catch (Exception e) {
      // Log but never fail the test for temp directory cleanup issues
      System.err.println("Warning: Exception during temp directory cleanup (ignored): " + e.getMessage());
    }
  }

  @Test public void testMaterializedViewsWithParquetEngine() throws Exception {
    skipIfEngineDoesNotSupportMaterializedViews();
    
    System.out.println("\n=== MATERIALIZED VIEWS WITH PARQUET ENGINE TEST ===");

    // Create model.json file with proper configuration
    File modelFile = new File(tempDir.toFile(), "model.json");
    try (FileWriter writer = new FileWriter(modelFile, StandardCharsets.UTF_8)) {
      writer.write("{\n");
      writer.write("  \"version\": \"1.0\",\n");
      writer.write("  \"defaultSchema\": \"PARQUET_MV_TEST\",\n");
      writer.write("  \"schemas\": [{\n");
      writer.write("    \"name\": \"PARQUET_MV_TEST\",\n");
      writer.write("    \"type\": \"custom\",\n");
      writer.write("    \"factory\": \"org.apache.calcite.adapter.file.FileSchemaFactory\",\n");
      writer.write("    \"operand\": {\n");
      writer.write("      \"directory\": \"" + tempDir.toString().replace("\\", "\\\\") + "\",\n");
      writer.write("      \"executionEngine\": \"parquet\",\n");
      writer.write("      \"ephemeralCache\": true,\n");
      writer.write("      \"materializations\": [{\n");
      writer.write("        \"view\": \"daily_summary\",\n");
      writer.write("        \"table\": \"daily_summary_mv\",\n");
      writer.write("        \"sql\": \"SELECT \\\"date\\\", COUNT(*) as transaction_count, SUM(\\\"quantity\\\") as total_quantity, SUM(\\\"quantity\\\" * \\\"price\\\") as total_revenue FROM sales GROUP BY \\\"date\\\"\"\n");
      writer.write("      }]\n");
      writer.write("    }\n");
      writer.write("  }]\n");
      writer.write("}\n");
    }

    // Use proper connection with model file
    try (Connection connection = DriverManager.getConnection("jdbc:calcite:model=" + modelFile.getAbsolutePath() + ";lex=ORACLE;unquotedCasing=TO_LOWER");
         Statement stmt = connection.createStatement()) {

      System.out.println("\n1. Created schema with Parquet engine and materialized view 'daily_summary' using model.json");

      // Check if .parquet file was expected
      File aperioDir = new File(tempDir.toFile(), ".aperio/PARQUET_MV_TEST");
      File mvParquetFile = new File(aperioDir, ".parquet_cache/.materialized_views/daily_summary_mv.parquet");
      System.out.println("\n2. Checking for materialized view Parquet file:");
      System.out.println("   Expected location: " + mvParquetFile.getAbsolutePath());
      System.out.println("   File exists: " + mvParquetFile.exists());

      // List all available tables
      System.out.println("\n3. Listing all tables in schema:");
      ResultSet tables =
          connection.getMetaData().getTables(null, "PARQUET_MV_TEST", "%", null);

      while (tables.next()) {
        String tableName = tables.getString("TABLE_NAME");
        System.out.println("   - " + tableName);
      }

      // Test LINQ4J with separate model
      System.out.println("\n4. Testing with non-Parquet engine:");
      File linq4jModelFile = new File(tempDir.toFile(), "model-linq4j.json");
      try (FileWriter writer = new FileWriter(linq4jModelFile, StandardCharsets.UTF_8)) {
        writer.write("{\n");
        writer.write("  \"version\": \"1.0\",\n");
        writer.write("  \"defaultSchema\": \"LINQ4J_MV_TEST\",\n");
        writer.write("  \"schemas\": [{\n");
        writer.write("    \"name\": \"LINQ4J_MV_TEST\",\n");
        writer.write("    \"type\": \"custom\",\n");
        writer.write("    \"factory\": \"org.apache.calcite.adapter.file.FileSchemaFactory\",\n");
        writer.write("    \"operand\": {\n");
        writer.write("      \"directory\": \"" + tempDir.toString().replace("\\", "\\\\") + "\",\n");
        writer.write("      \"executionEngine\": \"linq4j\",\n");
        writer.write("      \"ephemeralCache\": true,\n");
        writer.write("      \"materializations\": [{\n");
        writer.write("        \"view\": \"daily_summary\",\n");
        writer.write("        \"table\": \"daily_summary_mv\",\n");
        writer.write("        \"sql\": \"SELECT \\\"date\\\", COUNT(*) as transaction_count FROM sales GROUP BY \\\"date\\\"\"\n");
        writer.write("      }]\n");
        writer.write("    }\n");
        writer.write("  }]\n");
        writer.write("}\n");
      }
      
      try (Connection linq4jConn = DriverManager.getConnection("jdbc:calcite:model=" + linq4jModelFile.getAbsolutePath())) {
        System.out.println("   Created schema with LINQ4J engine and materializations");
        System.out.println("   Expected: Error message that MV only works with Parquet");
      }
    }
  }

}
