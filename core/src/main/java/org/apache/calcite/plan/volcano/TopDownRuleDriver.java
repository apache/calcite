/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to you under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.calcite.plan.volcano;

import org.apache.calcite.plan.Convention;
import org.apache.calcite.plan.DeriveMode;
import org.apache.calcite.plan.RelOptCluster;
import org.apache.calcite.plan.RelOptCost;
import org.apache.calcite.plan.RelOptRuleOperand;
import org.apache.calcite.plan.RelTraitSet;
import org.apache.calcite.rel.PhysicalNode;
import org.apache.calcite.rel.RelNode;
import org.apache.calcite.util.trace.CalciteTrace;

import org.checkerframework.checker.nullness.qual.Nullable;
import org.slf4j.Logger;

import java.util.ArrayList;
import java.util.Collection;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.Stack;

import static java.util.Objects.requireNonNull;

/**
 * A rule driver that applies rules in a Top-Down manner.
 * By ensuring rule applying orders, there could be ways for
 * space pruning and rule mutual exclusivity check.
 *
 * <p>This implementation uses tasks to manage rule matches.
 * A Task is a piece of work to be executed, it may apply some rules
 * or schedule other tasks.</p>
 */
@SuppressWarnings("JdkObsolete")
class TopDownRuleDriver implements RuleDriver {

  private static final Logger LOGGER = CalciteTrace.getPlannerTaskTracer();

  private final VolcanoPlanner planner;

  /**
   * The rule queue designed for top-down rule applying.
   */
  private final TopDownRuleQueue ruleQueue;

  /**
   * A scheduler to schedule tasks.
   */
  private final TaskScheduler scheduler;

  /**
   * RelNodes that are generated by {@link org.apache.calcite.rel.PhysicalNode#passThrough}
   * or {@link org.apache.calcite.rel.PhysicalNode#derive}. These nodes will not take part
   * in another passThrough or derive.
   */
  private final Set<RelNode> passThroughCache = new HashSet<>();

  /**
   * Whether to apply logical pruning in an eager mode.
   */
  private final boolean aggressivePruning;

  //~ Constructors -----------------------------------------------------------

  TopDownRuleDriver(VolcanoPlanner planner, Boolean aggressivePruning) {
    this.planner = planner;
    this.aggressivePruning = aggressivePruning;

    this.scheduler = new SimpleTaskScheduler();
    this.ruleQueue = new TopDownRuleQueue(planner);
  }

  //~ Methods ----------------------------------------------------------------

  @Override public void drive() {
    TaskDescriptor description = new TaskDescriptor();

    // Starting from the root's OptimizeGroup task.
    scheduler.schedule(
        new OptimizeGroup(
        requireNonNull(planner.root, "planner.root"), planner.infCost));

    // Ensure materialized view roots get explored.
    // Note that implementation rules or enforcement rules are not applied
    // unless the mv is matched.
    exploreMaterializationRoots();

    try {
      scheduler.drive(description);
    } catch (VolcanoTimeoutException ex) {
      LOGGER.warn("Volcano planning times out, cancels the subsequent optimization.");
    }
  }

  private void exploreMaterializationRoots() {
    for (RelSubset extraRoot : planner.explorationRoots) {
      RelSet rootSet = VolcanoPlanner.equivRoot(extraRoot.set);
      RelSubset root = requireNonNull(planner.root, "planner.root");
      if (rootSet == root.set) {
        continue;
      }
      if (rootSet != extraRoot.set) {
        extraRoot = requireNonNull(
            rootSet.getSubset(extraRoot.getTraitSet()), "extraRoot");
      }
      scheduler.schedule(new ExploreGroupExpressions(extraRoot));
    }
  }

  @Override public TopDownRuleQueue getRuleQueue() {
    return ruleQueue;
  }

  @Override public void clear() {
    ruleQueue.clear();
    scheduler.clear();
    passThroughCache.clear();
  }

  @Override public void onSetMerged(RelSet dest, RelSet src) {
    ruleQueue.onSetMerged(dest, src);

    // When RelSets get merged, an optimized group may get extra opportunities.
    // Clear the OPTIMIZED state for the RelSubsets and all theirs ancestors
    // so that they will be optimized again.
    // This should be rare because set merging typically happened
    // during exploration, when groups are usually not optimized.
    clearProcessed(dest);
  }

  private void clearProcessed(RelSet set) {
    for (RelSubset subset : set.subsets) {
      if (subset.resetTaskState()) {
        if (isDefaultTraits(subset)) {
          ruleQueue.recover(set);
        }
        Collection<RelNode> parentRels = subset.getParentRels();
        for (RelNode parentRel : parentRels) {
          RelSet parentRelSet =
              requireNonNull(planner.getSet(parentRel), () -> "no set found for " + parentRel);
          clearProcessed(parentRelSet);
        }
      }
    }
  }

  /**
   * Returns whether the input RelNode is has the default TraitSet
   * except Convention. It assumes they have the same traitDefs order.
   */
  private boolean isDefaultTraits(RelNode rel) {
    return rel.getCluster().traitSet()
        .equalsSansConvention(rel.getTraitSet());
  }

  //~ Inner Classes ----------------------------------------------------------

  /**
   * Base class for planner task.
   */
  private interface Task {
    void perform();
    void describe(TaskDescriptor desc);
  }

  /**
   * A class for task logging.
   */
  private static class TaskDescriptor {
    private boolean first = true;
    private final StringBuilder builder = new StringBuilder();

    void log(Task task) {
      if (!LOGGER.isDebugEnabled()) {
        return;
      }
      first = true;
      builder.setLength(0);
      builder.append("Execute task: ").append(task.getClass().getSimpleName());
      task.describe(this);
      if (!first) {
        builder.append(")");
      }

      LOGGER.debug(builder.toString());
    }

    TaskDescriptor item(String name, Object value) {
      if (first) {
        first = false;
        builder.append("(");
      } else {
        builder.append(", ");
      }
      builder.append(name).append("=").append(value);
      return this;
    }
  }

  /**
   * Optimize a RelSubset.
   *
   * This task is actually a wrapper of OptimizeGroupExpressions
   * for validating and ensuring default subset is optimized first.
   */
  private class OptimizeGroup implements Task {
    private final RelSubset group;
    private final RelOptCost upperBound;

    OptimizeGroup(RelSubset group, RelOptCost upperBound) {
      this.group = group;
      this.upperBound = upperBound;
    }

    @Override public void perform() {
      OptimizeGroupExpressions child = optimizeExpressions(group);
      if (child == null) {
        return;
      }
      scheduler.schedule(child);

      // If current physical properties is not for the default implementation
      // We first optimize the default implementation in order to avoid duplicate
      // exploring logical nodes and provide physical nodes for trait derivations
      if (!isDefaultTraits(group)) {
        RelOptCluster cluster = group.getCluster();
        RelTraitSet dftTraits = cluster.traitSet();

        Convention convention = group.getConvention();
        if (convention != null) {
          dftTraits = cluster.traitSet().replace(convention);
        }
        RelSubset defaultSubset;
        if (planner.isLogical(group.set.rel)) {
          defaultSubset = group.set.getOrCreateSubset(cluster, dftTraits, true);
        } else {
          defaultSubset = group.set.getSubset(dftTraits);
          if (defaultSubset == null) {
            return;
          }
        }
        OptimizeGroupExpressions optDefault = optimizeExpressions(defaultSubset);
        if (optDefault != null) {
          scheduler.schedule(optDefault);
        }
      }
    }

    @Nullable private OptimizeGroupExpressions optimizeExpressions(RelSubset group) {
      RelOptCost winner = group.getWinnerCost();
      if (winner != null) {
        return null;
      }

      if (group.taskState != null && upperBound.isLe(group.upperBound)) {
        // Either this group failed to optimize before or there is a ring
        return null;
      }

      group.startOptimize(upperBound);
      return new OptimizeGroupExpressions(group);
    }

    @Override public void describe(TaskDescriptor desc) {
      desc.item("group", group).item("upperBound", upperBound);
    }
  }

  /**
   * Schedule tasks to optimize the RelNodes inside the RelSet
   * and computes the winner of a RelSubset.
   *
   * Physical nodes always get higher priorities in order to achieve
   * tighter upper bounds as soon as possible.
   *
   * While handling logical nodes, it applies rule calls in the priorities
   * that substitute rules > build rules > exploration rules. It is a
   * simplified PROMISE mechanism in the cascades paper.
   *
   * Note that, when build rules are applied, it may produce new physical
   * nodes, who have a higher priority to be optimized. And when exploration
   * rules are invoked, it may produce some other logical nodes, whose build
   * rules will have higher priories.
   * It is difficult to model such priories with a simple stack scheduler.
   * Here we use OptimizeGroupExpressions to take full control over them.
   *
   * Each rule call will be checked against the upper bound to see whether
   * it is pruned.
   */
  private class OptimizeGroupExpressions implements Task {

    private final RelSubset group;
    private final boolean isDefault;

    @Nullable private TopDownRuleQueue.TopDownRuleMatchList matchList = null;
    @Nullable private List<PhysicalNode> nodesToPassThrough = null;
    @Nullable private List<PhysicalNode> nodesToDeriveTraits = null;
    @Nullable private List<RelNode> enforcers = null;

    // It relies on the fact that new rels added to a RelSubset must always
    // append to the tail of the list.
    private int processedExpr = 0;

    OptimizeGroupExpressions(RelSubset group) {
      this.group = group;
      this.isDefault = isDefaultTraits(group);
    }

    @Override public void perform() {
      RelSet equiv = this.group.set.equivalentSet;
      if (equiv != null) {
        while (equiv.equivalentSet != null) {
          equiv = equiv.equivalentSet;
        }
        if (planner.root.set == equiv) {
          // If roots are merged into another RelSet, we need to
          // reschedule root for optimization
          RelSubset newRoot = equiv.getSubset(group.getTraitSet());
          if (newRoot != null && newRoot.taskState == null) {
            scheduler.schedule(new OptimizeGroup(newRoot, group.upperBound));
          }
        }
        return;
      }

      // Process RelNodes inside the RelSet
      if (processMExprs() || exploreLogical()
          || passThroughTraits() || processEnforcers() || deriveTraits()) {
        return;
      }

      group.setOptimized();
    }

    private boolean deriveTraits() {
      // Derive traits again.
      if (nodesToDeriveTraits == null) {
        return false;
      }
      scheduler.schedule(this);
      for (PhysicalNode rel : nodesToDeriveTraits) {
        scheduler.schedule(new DeriveTraits(rel, group));
      }
      nodesToDeriveTraits = null;
      return true;
    }

    private boolean processEnforcers() {
      // Process enforcers to ensure their inputs are optimized.
      if (enforcers == null) {
        return false;
      }
      scheduler.schedule(this);
      for (RelNode rel : enforcers) {
        scheduler.schedule(new OptimizePhysicalMExpr(rel, group));
      }
      enforcers = null;
      return true;
    }

    private boolean passThroughTraits() {
      // Pass through traits for physical nodes
      if (nodesToPassThrough == null) {
        return false;
      }
      boolean modified = false;
      for (PhysicalNode rel : nodesToPassThrough) {
        RelNode ret = group.passThrough(rel);
        if (ret != null) {
          modified = true;
          planner.register(ret, rel);
        }
      }

      if (!modified) {
        return false;
      }

      // Mark new physical nodes with passThroughCache
      List<RelNode> rels = group.set.rels;
      for (int i = processedExpr; i < rels.size(); i++) {
        RelNode newRel = rels.get(i);
        if (newRel instanceof PhysicalNode) {
          passThroughCache.add(newRel);
        }
      }

      // In case new subsets are added as a new input,
      // derive traits again.
      if (nodesToDeriveTraits == null) {
        nodesToDeriveTraits = nodesToPassThrough;
      } else {
        nodesToDeriveTraits.addAll(nodesToPassThrough);
      }
      nodesToPassThrough = null;

      // Go through the nodes again and optimize new physical nodes
      scheduler.schedule(this);
      return true;
    }

    private boolean exploreLogical() {
      // Apply rules for logical nodes with the priorities
      // that substitute rules > build rules > exploration rules
      if (isDefault && matchList == null) {
        matchList = ruleQueue.getMatchList(this.group.set);
      }
      if (matchList == null) {
        return false;
      }

      // Substitution rules.
      List<VolcanoRuleMatch> substitute =
          matchList.get(TopDownRuleQueue.Promise.SUBSTITUTION);
      if (substitute != null && !substitute.isEmpty()) {
        scheduler.schedule(this);
        while (!substitute.isEmpty()) {
          // Substitution rules are never pruned.
          // So we just schedule all of them.
          // Note here these tasks could execute in parallel.
          VolcanoRuleMatch match = substitute.remove(0);
          scheduler.schedule(new ApplyRule(match, group));
        }
        return true;
      }

      // Implementation rules.
      List<VolcanoRuleMatch> impl =
          matchList.get(TopDownRuleQueue.Promise.IMPLEMENTATION);
      if (impl != null && !impl.isEmpty()) {
        scheduler.schedule(this);
        VolcanoRuleMatch match = impl.remove(impl.size() - 1);
        scheduler.schedule(new ApplyRule(match, group));
        // Only schedule one task at one time.
        return true;
      }

      List<VolcanoRuleMatch> explores =
          matchList.get(TopDownRuleQueue.Promise.EXPLORATION);
      if (explores != null && !explores.isEmpty()) {
        VolcanoRuleMatch match = explores.remove(0);
        scheduler.schedule(this);
        scheduler.schedule(new ApplyRule(match, group));
        // Only schedule one task at one time.
        return true;
      }
      return false;
    }

    private boolean processMExprs() {
      List<RelNode> rels = group.set.rels;
      while (processedExpr < rels.size()) {
        RelNode rel = rels.get(processedExpr++);
        if (planner.isLogical(rel)) {
          continue;
        }
        if (rel.getTraitSet().satisfies(group.getTraitSet())) {
          if (rel.isEnforcer()) {
            // Enforcers cannot be the best in default TraitSet.
            if (!isDefault && !(rel instanceof AbstractConverter)) {
              if (enforcers == null) {
                enforcers = new ArrayList<>();
              }
              // Save for later processing.
              enforcers.add(rel);
            }
            continue;
          }

          // If there are Physical nodes, it always gets higher priorities
          // in order to get a tighter upper bound as soon as possible.
          scheduler.schedule(this);
          scheduler.schedule(new OptimizePhysicalMExpr(rel, group));
          return true;
        } else if (!passThroughCache.contains(rel) && !rel.isEnforcer()) {
          if (rel instanceof PhysicalNode) {
            if (nodesToPassThrough == null) {
              nodesToPassThrough = new ArrayList<>();
            }
            nodesToPassThrough.add((PhysicalNode) rel);
          } else if (ruleQueue.getMatchForPhysicalNode(rel) != null) {
            // Happened only when a logical node is built to a relnode
            // that does not belong to the default RelSubset, perhaps
            // a different convention
            scheduler.schedule(this);
            scheduler.schedule(new ApplyRules(rel, group));
            return true;
          }
        }
      }
      return false;
    }

    @Override public void describe(TaskDescriptor desc) {
      desc.item("group", group)
          .item("processedExpr", processedExpr);
    }
  }

  /**
   * Task to apply a rule match.
   *
   * The rule match will be checked against the lower bound first.
   *
   * The lower bound is computed as the sum of all inputs' winner cost,
   * assuming that transformed target should always share the same
   * inputs with origin nodes.
   *
   * E.g. the rule matches
   *   rel1 --- rel2 --- rel3
   *      \____ rel4
   * The lower bound would be rel3.input.winnerCost + rel4.input.winnerCost.
   *
   * Note that if rel3.input's winner is missing, we treat it as ZeroCost, rather
   * than looking into rel3.input.input's winner recursively. That's because when
   * rel3's input is not fully optimized, there could be unfinished explorations.
   * So rel3.input.input's winnerCost is not necessary less than rel3.input's winnerCost.
   * Luckily, exploration rule matches are not vulnerable to this. That's because
   * build rules always have higher priorities, meaning that rel3.input should have
   * already been fully optimized when exploring rel1.
   */
  private class ApplyRule implements Task {

    private final VolcanoRuleMatch match;
    private final RelSubset group;

    ApplyRule(VolcanoRuleMatch match, RelSubset group) {
      this.match = match;
      this.group = group;
    }

    @Override public void perform() {
      if (ruleQueue.skipMatch(match)) {
        return;
      }
      if (checkLowerBound(match)) {
        match.onMatch();
      } else {
        ruleQueue.prune(group.set, match);
      }
    }

    @Override public void describe(TaskDescriptor desc) {
      desc.item("match", match);
    }

    private boolean checkLowerBound(VolcanoRuleMatch match) {
      if (group.upperBound.isInfinite()) {
        return true;
      }

      if (planner.isSubstituteRule(match)) {
        return true;
      }

      RelOptCost lb = getLowerBound(
          match.getRule().getOperand(), match.rels);
      if (lb == null) {
        return true;
      }
      if (lb.isInfinite()) {
        return false;
      }
      return lb.isLt(group.upperBound);
    }

    @Nullable private RelOptCost getLowerBound(RelOptRuleOperand op, RelNode[] rels) {
      List<RelOptRuleOperand> children = op.getChildOperands();
      if (children.isEmpty()) {
        return getLowerBound(rels[op.ordinalInRule]);
      }

      RelOptCost sum = null;
      for (RelOptRuleOperand child : children) {
        RelOptCost lb = getLowerBound(child, rels);
        if (lb != null) {
          if (lb.isInfinite()) {
            return lb;
          }
          sum = sum == null ? lb : sum.plus(lb);
        }
      }
      return sum;
    }

    @Nullable private RelOptCost getLowerBound(RelNode rel) {
      List<RelNode> inputs = rel.getInputs();
      if (inputs.isEmpty()) {
        return null;
      }

      RelOptCost sum = null;
      for (RelNode input : inputs) {
        if (!(input instanceof RelSubset)) {
          continue;
        }

        RelSubset inputSubset = (RelSubset) input;
        if (!aggressivePruning && !inputSubset.set.checkTransitive(planner)) {
          // If there could be transitive exploration, treat it as zero cost
          continue;
        }
        RelSubset defaultSubset = null;
        for (RelSubset subset : inputSubset.set.subsets) {
          if (subset.getConvention() == Convention.NONE) {
            continue;
          }
          if (isDefaultTraits(subset)) {
            defaultSubset = subset;
            break;
          }
        }
        if (defaultSubset == null
            || defaultSubset.taskState != RelSubset.OptimizeState.COMPLETED) {
          continue;
        }
        RelOptCost winner = defaultSubset.getWinnerCost();
        if (winner == null) {
          return planner.infCost;
        }
        sum = sum == null ? winner : sum.plus(winner);
      }
      return sum;
    }
  }

  /**
   * Optimize a physical node, including optimizing input and
   * derive traits from its optimized inputs. Traits are derived
   * only after all inputs have been fully optimized.
   */
  private class OptimizePhysicalMExpr implements Task {
    private final RelNode mExpr;
    private final RelSubset group;
    private final int inputCounts;

    OptimizePhysicalMExpr(RelNode mExpr, RelSubset group) {
      this.mExpr = mExpr;
      this.group = group;
      this.inputCounts = mExpr.getInputs().size();
    }

    private int optimizingInputOrdinal = -1;
    @Nullable private RelSubset optimizingInput = null;
    @Override public void perform() {
      if (inputCounts == 0) {
        return;
      }

      if (optimizingInput != null && optimizingInputOrdinal >= 0
          && mExpr.getInput(optimizingInputOrdinal) == optimizingInput
          && optimizingInput.getWinnerCost() == null) {
        // Failed to optimize the input group during last iteration
        return;
      }

      // Note that multiple inputs could be optimized in parallel as ORCA
      // does. But currently we do it in serial, taking better advantage of
      // the lower bound.
      optimizingInputOrdinal = -1;
      optimizingInput = null;
      List<RelNode> inputs = mExpr.getInputs();
      for (int i = 0; i < inputs.size(); i++) {
        RelSubset input = (RelSubset) inputs.get(i);
        if (input.getWinnerCost() == null) {
          optimizingInputOrdinal = i;
          optimizingInput = input;
          break;
        }
      }

      if (optimizingInput == null) {
        // In case some implementations use rules to convert between different
        // physical conventions. Note that this is deprecated and will be removed
        // in the future.
        if (ruleQueue.getMatchForPhysicalNode(mExpr) != null) {
          scheduler.schedule(new ApplyRules(mExpr, group));
        }

        // All inputs are optimized.
        if (mExpr instanceof PhysicalNode && !mExpr.isEnforcer()) {
          scheduler.schedule(new DeriveTraits((PhysicalNode) mExpr, group));
        }
        return;
      }

      RelOptCost upperBound = group.upperBound;
      if (!upperBound.isInfinite()) {
        RelOptCost lowerBound = null;
        if (!isDefaultTraits(group)) {
          lowerBound = group.getCluster()
              .getMetadataQuery().getNonCumulativeCost(mExpr);
        }
        for (RelNode input : mExpr.getInputs()) {
          RelOptCost winner = ((RelSubset) input).getWinnerCost();
          if (winner != null) {
            lowerBound = lowerBound == null ? winner : lowerBound.plus(winner);
          }
        }
        if (lowerBound != null) {
          if (upperBound.isLe(lowerBound)) {
            return;
          }
          upperBound = upperBound.minus(lowerBound);
        }
      }
      scheduler.schedule(this);
      scheduler.schedule(new OptimizeGroup(optimizingInput, upperBound));
    }

    @Override public void describe(TaskDescriptor desc) {
      desc.item("mExpr", mExpr).item("group", group);
    }
  }

  /**
   * A task to apply rules for a Node.
   * Currently, it is only used in mv exploration and
   * traits deriving with rule matches(deprecated).
   */
  private class ApplyRules implements Task {

    private final RelNode mExpr;
    private final RelSubset group;

    ApplyRules(RelNode mExpr, RelSubset group) {
      this.mExpr = mExpr;
      this.group = group;
    }

    @Override public void perform() {
      List<VolcanoRuleMatch> matches = ruleQueue.getMatchForPhysicalNode(mExpr);
      if (matches != null && !matches.isEmpty()) {
        VolcanoRuleMatch match = matches.remove(matches.size() - 1);
        scheduler.schedule(this);
        scheduler.schedule(new ApplyRule(match, group));
      }
    }

    @Override public void describe(TaskDescriptor desc) {
      desc.item("mExpr", mExpr).item("group", group);
    }
  }

  /**
   * Derive traits according to PhysicalNode's derive mode.
   * Note that when this method is invoked, all inputs of
   * the physical node should have been optimized already.
   */
  private class DeriveTraits implements Task {

    private final PhysicalNode mExpr;
    private final RelSubset group;
    private final int exprCntBeforeDerive;

    DeriveTraits(PhysicalNode mExpr, RelSubset group) {
      this.mExpr = mExpr;
      this.group = group;
      exprCntBeforeDerive = group.set.rels.size();
    }

    @Override public void perform() {
      DeriveMode deriveMode = mExpr.getDeriveMode();
      if (deriveMode == DeriveMode.PROHIBITED) {
        return;
      }

      if (passThroughCache.contains(mExpr)) {
        return;
      }

      if (mExpr.getInputs().stream()
          .anyMatch(i -> ((RelSubset) i).getWinnerCost() == null)) {
        return;
      }
      RelOptCost ub = group.upperBound;
      if (!ub.isInfinite()) {
        RelOptCost cost = planner.getCost(
            mExpr, mExpr.getCluster().getMetadataQuery());
        if (cost == null || ub.isLt(cost)) {
          return;
        }
      }

      if (deriveMode == DeriveMode.OMAKASE) {
        omakase();
      } else {
        List<RelNode> inputs = mExpr.getInputs();
        int childrenCount = inputs.size();
        for (int i = 0; i < childrenCount; i++) {
          int childId = i;
          if (deriveMode == DeriveMode.RIGHT_FIRST) {
            childId = childrenCount - childId - 1;
          }

          RelNode input = inputs.get(childId);
          List<RelSubset> toDerive = null;
          for (RelSubset s : ((RelSubset) input).set.subsets) {
            if (!needsToDerive(s) || !s.derivedBy(mExpr)) {
              continue;
            }
            if (toDerive == null) {
              toDerive = new ArrayList<>();
            }
            toDerive.add(s);
          }
          if (toDerive != null) {
            for (RelSubset subset : toDerive) {
              derive(mExpr, subset, childId);
            }
          }
          if (deriveMode != DeriveMode.BOTH) {
            break;
          }
        }
      }

      // Cache the new nodes in passThroughCache. Those nodes will
      // not take part in further passThrough or derive calls.
      List<RelNode> rels = group.set.rels;
      for (int i = exprCntBeforeDerive; i < rels.size(); i++) {
        RelNode newNode = rels.get(i);
        if (newNode instanceof PhysicalNode) {
          passThroughCache.add(newNode);
        }
      }
    }

    private void omakase() {
      boolean empty = true;
      List<List<RelTraitSet>> inputTraits = new ArrayList<>(mExpr.getInputs().size());
      for (RelNode input : mExpr.getInputs()) {
        List<RelTraitSet> traits = new ArrayList<>();
        inputTraits.add(traits);

        RelSubset inputSubset = (RelSubset) input;
        for (RelSubset subset : inputSubset.set.subsets) {
          if (needsToDerive(subset) && subset.getWinnerCost() != null) {
            traits.add(subset.getTraitSet());
            empty = false;
          }
        }
      }

      if (empty) {
        return;
      }

      List<RelNode> relList = mExpr.derive(inputTraits);
      for (RelNode relNode : relList) {
        if (!planner.isRegistered(relNode)) {
          planner.register(relNode, mExpr);
        }
      }
    }

    private void derive(PhysicalNode mExpr, RelSubset child, int childId) {
      RelNode newRel = mExpr.derive(child.getTraitSet(), childId);
      if (newRel == null || planner.isRegistered(newRel)) {
        return;
      }
      RelNode newInput = newRel.getInput(childId);
      assert newInput instanceof RelSubset;
      if (newInput == child) {
        // If the child subset is used to derive new traits for
        // current relnode, the subset will be marked REQUIRED
        // when registering the new derived relnode and later
        // will add enforcers between other delivered subsets.
        // e.g. a MergeJoin request both inputs hash distributed
        // by [a,b] sorted by [a,b]. If the left input R1 happens to
        // be distributed by [a], the MergeJoin can derive new
        // traits from this input and request both input to be
        // distributed by [a] sorted by [a,b]. In case there is an
        // alternative R2 with ANY distribution in the left input's
        // RelSet, we may end up with requesting hash distribution
        // [a] on alternative R2, which is unnecessary and waste,
        // because we request distribution by [a] because of R1 can
        // deliver the exact same distribution, and we don't need to
        // enforce properties on other subsets that can't satisfy
        // the specific trait requirement.
        // Here we add a constraint that {@code newInput == subset},
        // because if the delivered child subset is HASH[a], but
        // we require HASH[a].SORT[a,b], we still need to enable
        // property enforcement on the required subset. Otherwise,
        // we need to restrict enforcement between HASH[a].SORT[a,b]
        // and HASH[a] only, which will make things a little more
        // complicated. We might optimize it in the future.
        child.disableEnforcing();
      }
      planner.register(newRel, mExpr);
    }

    boolean needsToDerive(RelSubset subset) {
      // Ideally we should stop deriving new RelNodes when the
      // subset's traitSet equals with input traitSet, but
      // in case someone manually builds a physical relnode
      // tree, which is highly discouraged, without specifying
      // correct traitSet, e.g.
      //   EnumerableFilter  [].ANY
      //       -> EnumerableMergeJoin  [a].Hash[a]
      // We should still be able to derive the correct traitSet
      // for the dumb filter, even though the filter's traitSet
      // should be derived from the MergeJoin when it is created.
      // But if the subset's traitSet equals with the default
      // empty traitSet sans convention (the default traitSet
      // from cluster may have logical convention, NONE, which
      // is not interesting), we are safe to ignore it, because
      // a physical filter with non default traitSet, but has an
      // input with default empty traitSet, e.g.
      //   EnumerableFilter  [a].Hash[a]
      //       -> EnumerableProject  [].ANY
      // is definitely wrong, we should fail fast.
      return subset.isDelivered() && !isDefaultTraits(subset);
    }

    @Override public void describe(TaskDescriptor desc) {
      desc.item("mExpr", mExpr).item("group", group);
    }
  }

  /**
   * A task used for exploring root of materialized views.
   */
  private class ExploreGroupExpressions implements Task {
    private final RelSubset group;

    private int processingExpr = 0;
    ExploreGroupExpressions(RelSubset group) {
      this.group = group;
    }

    @Override public void perform() {
      if (group.taskState != null) {
        return;
      }
      group.startOptimize(planner.infCost);

      List<RelNode> rels = group.set.rels;
      while (processingExpr < rels.size()) {
        RelNode rel = rels.get(processingExpr++);
        if (!planner.isLogical(rel)) {
          continue;
        }
        scheduler.schedule(this);
        for (RelNode input : rel.getInputs()) {
          scheduler.schedule(new ExploreGroupExpressions((RelSubset) input));
        }
      }
      TopDownRuleQueue.TopDownRuleMatchList matches = ruleQueue.getMatchList(group.set);
      List<VolcanoRuleMatch> substitutes = matches.get(TopDownRuleQueue.Promise.SUBSTITUTION);
      if (substitutes != null && !substitutes.isEmpty()) {
        scheduler.schedule(this);
        while (!substitutes.isEmpty()) {
          VolcanoRuleMatch match = substitutes.remove(0);
          scheduler.schedule(new ApplyRule(match, group));
        }
        return;
      }
      List<VolcanoRuleMatch> explores = matches.get(TopDownRuleQueue.Promise.EXPLORATION);
      if (explores != null && !explores.isEmpty()) {
        scheduler.schedule(this);
        scheduler.schedule(new ApplyRule(explores.remove(0), group));
        return;
      }

      // If RelSets get merged, the optimized flag would be cleared
      group.setOptimized();
    }

    @Override public void describe(TaskDescriptor desc) {
      desc.item("group", group);
    }
  }

  /**
   * Interface for task scheduler.
   */
  interface TaskScheduler {
    void schedule(Task task);
    void drive(TaskDescriptor descriptor);
    void clear();
  }

  /**
   * A simple implementation of task scheduler, using a stack to manage tasks.
   */
  private static class SimpleTaskScheduler implements TaskScheduler {
    private final Stack<Task> tasks = new Stack<>();

    @Override public void schedule(Task task) {
      tasks.push(task);
    }

    @Override public void clear() {
      tasks.clear();
    }

    @Override public void drive(TaskDescriptor descriptor) {
      while (!tasks.isEmpty()) {
        Task task = tasks.pop();
        descriptor.log(task);
        task.perform();
      }
    }
  }
}
